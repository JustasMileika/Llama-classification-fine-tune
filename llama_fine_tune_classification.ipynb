{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrAHTiglX1Dl"
   },
   "source": [
    "# About\n",
    "\n",
    "This notebook contains code for fine-tuning Llama-3.2-1B model for text classification to sections.\n",
    "\n",
    "The texts are classified to the top 10 most frequent sections and the rest are classified as \"Other\"\n",
    "\n",
    "Fine-tuning is done using QLORA.\n",
    "\n",
    "The resulting model is integrated into Question Answering pipeline using Haystack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En8tAZBOH69C"
   },
   "source": [
    "Code is intended to run on a cuda architecture\n",
    "\n",
    "All code was compiled using A100 GPU on google colab\n",
    "\n",
    "Loading up and training the model takes < 20 GB of GPU RAM (so L4 GPU could be used as well, but the training will be slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Jksu_2eGdPXl",
    "outputId": "bc6ab060-e699-48bf-c339-6d34d6fafca8"
   },
   "outputs": [],
   "source": [
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLm-TuPU35jk"
   },
   "source": [
    "Log in to Hugging Face\n",
    "\n",
    "Needed for loading models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE4oSROP34c4"
   },
   "outputs": [],
   "source": [
    "hf_token = \"<hf_token>\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lui22HafL4y4"
   },
   "source": [
    "Mount Google Drive\n",
    "\n",
    "Used for saving and loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8ZLsaTCL3Sp",
    "outputId": "c98fda13-b2c2-4eb9-96d3-e8deb3f958b4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVDPscLjLKnD",
    "outputId": "c7e4e9d4-e22a-4082-87d8-c43d8678b280"
   },
   "outputs": [],
   "source": [
    "!pip install haystack-ai datasets transformers\n",
    "!pip install -U bitsandbytes\n",
    "!pip install evaluate\n",
    "\n",
    "!pip install haystack-ai\n",
    "!pip install chroma-haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-Apk9sfx-HI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig, PeftModel\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from haystack import component\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.generators.chat import HuggingFaceAPIChatGenerator\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.utils import Secret\n",
    "from haystack.utils.hf import HFGenerationAPIType\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "0c25e2fd2d694f2982b52c21da9ea392",
      "d8c7061680a5489f8c30b0d66be1208e",
      "80deeb9b66bc42b08d40a2e93e4e873e",
      "7ac37b1e1ac64af4a7a654a1584e6d89",
      "83adeda6bc994d4c9d52c2ecd501e6bf",
      "a74a4487d0664105b7075bd6551d1c67",
      "d105d71f31084807aae37ab093c35690",
      "9113af3952044f40b075258af2dc6581",
      "c5237f263d7945adaf57dda427ac6081",
      "6bc5ccbd6b6a403a960830cc114a333a",
      "eea8a2ac66a84576b2071601c658a916",
      "91f49cf596db4ae3b2c86f98d2f13935",
      "d374921c5c644d5485bb01c9c55a290c",
      "717fbaafb76e41a485efb2810195ca4c",
      "f710a7c4f8714d98abc0bd9541af59b9",
      "4d71aac968eb4526ba26177529247939",
      "66b3eeed38fb4e94a5918f94e8ad22f2",
      "c74903cb04fe4a98b473be2dbcf5a233",
      "af4c077d343e4b7b9aa997f7173cd6c1",
      "f8a0e10a67b74f1d928434da15b86828",
      "44bc74c3ff48459ea40e58e9172f865d",
      "599050619936423798f026f504f43f41",
      "8e65f87cb5c444fb8c42cf59311d30e4",
      "7f010eb99fbe474781d17babf443542e",
      "5d257202d98643d4961a7920899965e9",
      "68a06ad05ef846ce9a4e2b0af1040f74",
      "d02c4b10e81d48b293d445b00a7f2320",
      "33f50a7755b04f8ca913512f84b3cf36",
      "5efefe746af546339170624bb2de6c9b",
      "7c211e80139c4127ae779030bc45794a",
      "8f9452d81b2840cbb415d2f40559806a",
      "7efe920a8f77497eb31687a4777370a1",
      "8054cc59d67d4672ab81e51b3985003d",
      "1ccd94a37f6c47fb86d2177cd57343c6",
      "a57e80159d114725afa85382e54bfbe0",
      "1b051ffeeedd428ea97d0a1d3ac84430",
      "a76356bb0f4840aaab36c5d306e4adb7",
      "04b99c6b44894d3c814efedeb29a2a83",
      "d479545e6d464f5e8f96b9d40d3ee694",
      "8dd96e3e3a4f4e92a181cc9a49ca2bba",
      "3253e6ed18314695940cea7ee7bfb9bf",
      "f8d48a48452b4d5bbe5718272853812b",
      "3abeb4cf25b149718123eea6c1ef5b89",
      "9fc428ddb308424f86eaa8827197c652",
      "4646cce218d244ddb78fe5dc7e980f6f",
      "bf8abea98ebf445e95d6a3cadc1720f7",
      "b90a084a045548258d64326e3f4703f1",
      "808f9d7d03b241538adf41253f12d942",
      "f7505b0f7ccb4535a20d5c070c1a44e4",
      "ece4fc088ab447b0a20884cd4d89c77b",
      "9e300a4373cd4208a760141e602a8515",
      "124d4272de7547d49963394d841751a9",
      "0e47347c16c44764817db8aa2c40151a",
      "a268e18a25d04dfd9a10132f2f0085f4",
      "1c58b4d8bced40cb8f1e62cc319d7a5d",
      "11471fbb19264971a67d45af7efa02dd",
      "f2486b09383240b5b61236c26484bee3",
      "29d6c4786ffd4411b4a3c7f1e570c1f3",
      "f8138338e7e04aadbbffa4e0ab0b599c",
      "267f081a765a492390d765e7e251bdda",
      "495e0abade7540989da671b267afc3cb",
      "1de3eb6bb56f415392314f7a79588379",
      "716c4c06e2114006983a2a0dbc3f48f1",
      "a22dfa35530a46c3aa6cb713b0cd7cb4",
      "d32a70ab2bd049f7be05587b905b639e",
      "e64c0a3b50a04bba927bcbb3037955dd"
     ]
    },
    "id": "elCdC8kYLy37",
    "outputId": "c1aee6ba-73cc-47ba-f61a-99f6ab292c89"
   },
   "outputs": [],
   "source": [
    "loaded_dataset = load_dataset(\"Justelioo/text-section-classification\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_wUIX4C-xd4"
   },
   "source": [
    "Calculate top sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zPvtZj4jjMN"
   },
   "outputs": [],
   "source": [
    "sections_counts = Counter()\n",
    "\n",
    "for sample in loaded_dataset:\n",
    "  sections_counts[sample[\"section\"]] += 1\n",
    "\n",
    "top_section_counts = dict(sections_counts.most_common(10))\n",
    "section_list = list(top_section_counts.keys())\n",
    "top_sections = set(section_list)\n",
    "section_list.append(\"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtgXEQP4przi",
    "outputId": "f1121957-a093-4eb0-9330-975d275a141e"
   },
   "outputs": [],
   "source": [
    "section_to_index = {section: idx for idx, section in enumerate(section_list)}\n",
    "index_to_section = {idx: section for idx, section in enumerate(section_list)}\n",
    "print(section_to_index)\n",
    "print(index_to_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KczlJ6EbTlpr"
   },
   "source": [
    "Inspect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o70yzn8oTFQG",
    "outputId": "c692b20d-2642-4192-d5da-997c08d88822"
   },
   "outputs": [],
   "source": [
    "loaded_dataset.filter(lambda example: example[\"section\"] == \"Types\", num_proc=16)[45:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPefNEANONQS"
   },
   "outputs": [],
   "source": [
    "def load_data(other_data_count, seed=7):\n",
    "\n",
    "  other_data = loaded_dataset.filter(lambda example: example[\"section\"] not in top_sections, num_proc=16)\n",
    "  top_section_data = loaded_dataset.filter(lambda example: example[\"section\"] in top_sections, num_proc=16)\n",
    "\n",
    "  selected_other_data = other_data.select(range(other_data_count))\n",
    "\n",
    "  dataset = concatenate_datasets([top_section_data, selected_other_data])\n",
    "  dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "  dataset = dataset.train_test_split(train_size=0.8, seed=seed)\n",
    "\n",
    "  test_dataset = dataset.pop(\"test\")\n",
    "\n",
    "  test_validation_dataset = test_dataset.train_test_split(train_size=0.5, seed=seed)\n",
    "\n",
    "  dataset[\"validation\"] = test_validation_dataset[\"train\"]\n",
    "  dataset[\"test\"] = test_validation_dataset[\"test\"]\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2pZ8bl498Pf"
   },
   "outputs": [],
   "source": [
    "def prepare_tokenizer(checkpoint):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cYAvmkq_FET"
   },
   "outputs": [],
   "source": [
    "def process_data(dataset, tokenizer, truncation=True, max_length=512):\n",
    "  def tokenize(example):\n",
    "    tokenized = tokenizer(example['text'], truncation=truncation, max_length=max_length)\n",
    "    tokenized['label'] = [section_to_index.get(section, 10) for section in example['section']]\n",
    "    return tokenized\n",
    "\n",
    "  tokenized = dataset.map(tokenize, batched=True, num_proc=16, remove_columns=['title', 'section', 'text'])\n",
    "  return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOWhBz2-5Im"
   },
   "source": [
    "Define QLora configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxnYrCno8qTm"
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type='nf4',\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16\n",
    "  )\n",
    "\n",
    "num_labels = 11\n",
    "\n",
    "def prepare_model(checkpoint, tokenizer):\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      checkpoint,\n",
    "      quantization_config=quantization_config,\n",
    "      num_labels = num_labels,\n",
    "      device_map=\"auto\",\n",
    "      )\n",
    "\n",
    "  lora_config = LoraConfig(\n",
    "      r = 32,\n",
    "      lora_alpha = 32,\n",
    "      target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "      lora_dropout = 0.05,\n",
    "      bias = 'none',\n",
    "      task_type='SEQ_CLS',\n",
    "  )\n",
    "\n",
    "  prepared_model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "  peft_model = get_peft_model(prepared_model, lora_config)\n",
    "\n",
    "  peft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "  peft_model.config.use_cache = False\n",
    "  peft_model.config.pretraining_tp = 1\n",
    "\n",
    "  return peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCT_pCSu_CY_"
   },
   "source": [
    "Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMcCjPg7QV_u"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load('f1')\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "  accuracy_metric = accuracy.compute(predictions = predictions, references=labels)\n",
    "  precision_metric = precision.compute(predictions = predictions, references=labels, average=\"macro\")\n",
    "  recall_metric = recall.compute(predictions = predictions, references=labels, average=\"macro\")\n",
    "  f1_metric = f1.compute(predictions = predictions, references=labels, average=\"macro\")\n",
    "  cm = confusion_matrix(labels, predictions)\n",
    "\n",
    "  return {\n",
    "      \"accuracy\": accuracy_metric[\"accuracy\"],\n",
    "      \"precision\": precision_metric[\"precision\"],\n",
    "      \"recall\": recall_metric[\"recall\"],\n",
    "      \"f1\": f1_metric[\"f1\"],\n",
    "  }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return calculate_metrics(predictions, labels)\n",
    "\n",
    "def evaluate_model(trainer, dataset, class_names):\n",
    "  preds = trainer.predict(dataset)\n",
    "  labels = preds.label_ids\n",
    "  predictions = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "  cm = confusion_matrix(labels, predictions, normalize='true')\n",
    "\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues',\n",
    "              xticklabels=class_names,\n",
    "              yticklabels=class_names)\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('True')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "\n",
    "  return calculate_metrics(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmF7vq00_FFX"
   },
   "source": [
    "Define custom Trainer class to support loss with class weights\n",
    "\n",
    "Intended to counteract inbalanced dataset sections\n",
    "\n",
    "Ablations suggest class weights impact performance negatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTpMOxma7XRl"
   },
   "outputs": [],
   "source": [
    "class TrainerWithClassWeights(Trainer):\n",
    "  def __init__(self, *args, class_weights, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.class_weights = class_weights.to(\"cuda\")\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs['logits']\n",
    "    labels = inputs['labels']\n",
    "\n",
    "    loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def prepare_trainer(model, tokenizer, dataset, output_dir=\"trained_model\", class_weights=None):\n",
    "  training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "  )\n",
    "\n",
    "  collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "\n",
    "  if class_weights is not None:\n",
    "    return TrainerWithClassWeights(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = dataset['train'],\n",
    "      eval_dataset = dataset['validation'],\n",
    "      processing_class=tokenizer,\n",
    "      data_collator=collator,\n",
    "      compute_metrics=compute_metrics,\n",
    "      class_weights=class_weights,\n",
    "    )\n",
    "  else:\n",
    "    return Trainer(\n",
    "      model = model,\n",
    "      args = training_args,\n",
    "      train_dataset = dataset['train'],\n",
    "      eval_dataset = dataset['validation'],\n",
    "      processing_class=tokenizer,\n",
    "      data_collator=collator,\n",
    "      compute_metrics=compute_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73QaS3HD1vuI"
   },
   "outputs": [],
   "source": [
    "def calculate_class_weights(other_data_count):\n",
    "\n",
    "  top_section_counts['Other'] = other_data_count\n",
    "\n",
    "  counts = torch.tensor(list(top_section_counts.values()))\n",
    "\n",
    "  total_count = counts.sum()\n",
    "  frequencies = counts / total_count\n",
    "\n",
    "  class_weights = 1 / frequencies\n",
    "  class_weights_sum = class_weights.sum()\n",
    "  class_weights = class_weights / class_weights_sum\n",
    "  return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16MuerD4PX_L"
   },
   "outputs": [],
   "source": [
    "checkpoint = \"meta-llama/Llama-3.2-1B\"\n",
    "model_save_dir = \"drive/MyDrive/final/1b-512cl-nocw-other50k\"\n",
    "other_data_count = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lky2Ae-YhJn_",
    "outputId": "fedbb619-aea1-4871-9732-4392b987725f"
   },
   "outputs": [],
   "source": [
    "dataset = load_data(other_data_count=other_data_count)\n",
    "tokenizer = prepare_tokenizer(checkpoint)\n",
    "tokenized = process_data(dataset, tokenizer)\n",
    "model = prepare_model(checkpoint, tokenizer)\n",
    "# class_weights = calculate_class_weights(other_data_count)\n",
    "trainer = prepare_trainer(model, tokenizer, tokenized, model_save_dir, class_weights=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX9DPRpG_VAy"
   },
   "source": [
    "Calculate data token count quantiles to choose suitable truncation length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0gQ6PVT6hbY",
    "outputId": "8ec1a020-31f4-407e-ab5d-8d9f81f183cd"
   },
   "outputs": [],
   "source": [
    "untruncated_data = process_data(dataset, tokenizer, truncation=False)\n",
    "lengths = untruncated_data.map(lambda example: {\"token_length\": [len(ids) for ids in example['input_ids']]}, batched=True, num_proc=16)\n",
    "\n",
    "quantiles = np.quantile(lengths[\"test\"]['token_length'], [0, 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 1])\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPGDvlyyAh95"
   },
   "source": [
    "Evaluate model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7jq7FlBbLnH6",
    "outputId": "a539e650-06e9-490c-85de-494f3ebaf103"
   },
   "outputs": [],
   "source": [
    "evaluate_model(trainer, tokenized[\"test\"], section_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "id": "iz59mjtgZqG5",
    "outputId": "97ea3c05-59c2-4c2f-fa8b-ff9023da9746"
   },
   "outputs": [],
   "source": [
    "trained = trainer.train()\n",
    "trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg4auzI7A4dn"
   },
   "source": [
    "Evaluate on test set after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "9nYfVF-SPdeR",
    "outputId": "ac094f3e-26fa-4042-aa75-09be86a917ee"
   },
   "outputs": [],
   "source": [
    "evaluate_model(trainer, tokenized[\"test\"], section_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwqNcTS_A66-"
   },
   "source": [
    "Evaluating on bigger dataset (with more 'Other' section data)\n",
    "\n",
    "Used for evaluating the hyperparameter `other_data_count`, since choosing different values yields different `test` dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956,
     "referenced_widgets": [
      "0e152b4d1eec46848ae736a163dd2ddb",
      "27e44f2369584bb587507f53f76b3c9c",
      "deac1721cf1d469186b30405387fd994",
      "87354e96371347b5afd12f157e9eba3c",
      "16e480c6a58b4b51bec8a6b976250948",
      "d1ccffd29f2a4e7e910a8f34801792e8",
      "28ea9ed9fba14fd3bb006ae889f59c86",
      "aa90d2207a7b47a79ee3d0af6669b154",
      "91107117bf274579aaadc8bcec397795",
      "669cd52f77954cdbb5dfae47b15f62cd",
      "9ae01aa3c72a4f759901221ad41dad29",
      "1975e5a4d5c14eafa25449c31b19c93c",
      "172b346fa5ab4634af4cfd09f3a187cd",
      "2e5d40647e5a4fefa34bda55e102839e",
      "b0d8ec4fe7324321b9ced022f51cacc9",
      "9d82144a67434f6c936b354515818983",
      "428fc5177b184d188fa4ea54573effe8",
      "fee432d9713e43fcb13c045b36083c85",
      "26897f154c0d4524950ff5aab559c36e",
      "81a73fc4f4be4a818599898135b53dc5",
      "1c13a101294d4cbabc9bec9a4cf5c889",
      "b620bd7e6dca4584b35899b08c8a87ea",
      "809d073df6c1464d829270aa1122a934",
      "65ceeb8fb03c41f2a43c05d78b2671bb",
      "ba49882e17f14121be94e0be6a23a07a",
      "2dc647b06db448c3898a9f3fc410eafc",
      "8fa86301c96d43db88eda18027945c64",
      "77f610d1dc5e42b785c42a8f77dcb9f6",
      "0e1820b25a774d7389de5c4bc9c7bf28",
      "366b0a7715f6441ea26923d35539cacb",
      "9c26027323c04f57bdc2430b156e2741",
      "504b5fcb600b47209d8937a1278f846f",
      "ca1467cf8be34c3ea4ac577e7859d486"
     ]
    },
    "id": "YZzFkWp1T81h",
    "outputId": "79513bae-371d-4209-8081-4205b3b28a51"
   },
   "outputs": [],
   "source": [
    "bigger_dataset = load_data(other_data_count=200000)\n",
    "bigger_tokenized = process_data(bigger_dataset, tokenizer)\n",
    "evaluate_model(trainer, bigger_tokenized[\"test\"], section_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUdSQGN9DNgE"
   },
   "outputs": [],
   "source": [
    "def save_trained(save_directory, trainer, tokenizer):\n",
    "  trainer.save_model(save_directory)\n",
    "  model.config.save_pretrained(save_directory)\n",
    "  tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_JvhtUdB2Gw"
   },
   "source": [
    "Save best loaded model after training to a different folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACnmwoOXEsln"
   },
   "outputs": [],
   "source": [
    "save_directory = f\"{model_save_dir}-final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRMnyEtHDira"
   },
   "outputs": [],
   "source": [
    "save_trained(save_directory, trainer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH6CkOBDR-f8"
   },
   "outputs": [],
   "source": [
    "def load_saved_model(save_directory, tokenizer):\n",
    "  peft_config = PeftConfig.from_pretrained(save_directory)\n",
    "\n",
    "  base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=num_labels,\n",
    "    device_map=\"auto\",\n",
    "  )\n",
    "\n",
    "  peft_model = PeftModel.from_pretrained(base_model, save_directory)\n",
    "\n",
    "  peft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "  return peft_model\n",
    "\n",
    "\n",
    "def load_saved_tokenizer(save_directory):\n",
    "  return AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPHfGNgAB9vE"
   },
   "source": [
    "Loading and evaluating saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6baff5d6d75b4745bb689d08f00a638b",
      "79c8c55b94aa4adc925390cebf94f23b",
      "cfb307aa9e2e4f83837e495f5bf39334",
      "ab02bc2201d04890aea7623654f7300b",
      "7d66bc0ad2ad4a5aa6458331b0133320",
      "822866e029394387856fcdee15a3bb5d",
      "176ddca9528e40f8ad2ad3b6a171ab1d",
      "2685eaf0d9bf497da0615c1a05c8c7e3",
      "8734ea2ed9434f11831f796633e41a13",
      "44613fb3caac4821af41f39ac5de7028",
      "e113fa9883ca435bab1d8cf356d29908",
      "5fcd9ca0e96e4dab9fa4c6825b0e469e",
      "61b0bd3dfc714fce92090d90bfa93e64",
      "fb7b7a9866f5422dabc9ca38fe9a6c3e",
      "80450db137804a50ba0f49d78ef699dd",
      "ede115dab30043f780919bd03f0f91e9",
      "40fe195bab4f4ce88058c760c8be33db",
      "51c60bf397f34dc1a3fbc6197679816c",
      "acf690e7538e4bb49783909cfac60eae",
      "034de8ae20e24d5db75a1303cce34f02",
      "c795999667e9401b9929b9bb9e0f9cfd",
      "26471eb615664560babcc588e4846de2",
      "5c8d7f24ff134d849c3a995e284edc66",
      "4686ae5f1ae74e8f87b117db3dd65504",
      "fb3f2e10fc104083aec67bd888f866e5",
      "ec28a8bcd2664169bf5164ec3cfb19c9",
      "a9a1a8044da145a9ab26e669b2a2ecaf",
      "a2ad43df6b544629ae38f11e4a23519e",
      "32deff86affc44a3b67addfb294d46cf",
      "68504433bb2849ca995942705fd31d43",
      "52d0e885e1ce440d83d24eb62c17eeb4",
      "5d8f802a1db946ca8196eabedc043131",
      "a934335c690942198a6c66abff031e5b"
     ]
    },
    "id": "cGHMhfXeVSt-",
    "outputId": "68a90352-f95c-4a76-d994-8ae31de27028"
   },
   "outputs": [],
   "source": [
    "dataset = load_data(other_data_count=200000)\n",
    "loaded_tokenizer = load_saved_tokenizer(save_directory)\n",
    "loaded_model = load_saved_model(save_directory, loaded_tokenizer)\n",
    "tokenized = process_data(dataset, loaded_tokenizer)\n",
    "loaded_trainer = prepare_trainer(loaded_model, loaded_tokenizer, tokenized)\n",
    "evaluate_model(loaded_trainer, tokenized[\"test\"], section_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-HDJ7JehaRb"
   },
   "source": [
    "Defining custom haystack components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7n-6gaSp9rUl"
   },
   "outputs": [],
   "source": [
    "@component\n",
    "class DocumentClassification:\n",
    "\n",
    "  def __init__(self, model_dir):\n",
    "    self.model_dir = model_dir\n",
    "\n",
    "  def _load_model(self, directory, tokenizer):\n",
    "    peft_config = PeftConfig.from_pretrained(directory)\n",
    "\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      peft_config.base_model_name_or_path,\n",
    "      quantization_config=quantization_config,\n",
    "      num_labels=num_labels,\n",
    "      device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    peft_model = PeftModel.from_pretrained(base_model, directory)\n",
    "\n",
    "    peft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "  def _load_tokenizer(self, directory):\n",
    "    return AutoTokenizer.from_pretrained(directory)\n",
    "\n",
    "  def warm_up(self):\n",
    "    self.tokenizer = self._load_tokenizer(self.model_dir)\n",
    "    self.model = self._load_model(self.model_dir, self.tokenizer)\n",
    "\n",
    "\n",
    "  @component.output_types(documents=List[Document])\n",
    "  def run(self, documents: List[Document]):\n",
    "\n",
    "    contents = [document.content for document in documents]\n",
    "\n",
    "    tokenized = self.tokenizer(contents, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = self.model(**tokenized)\n",
    "      logits = outputs.logits\n",
    "      labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "      documents[idx].meta[\"classification\"] = label.item()\n",
    "\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "@component\n",
    "class TextToDocumentConverter:\n",
    "\n",
    "    @component.output_types(document=Document)\n",
    "    def run(self, text: str):\n",
    "        return {\"document\": Document(content=text)}\n",
    "\n",
    "@component\n",
    "class DocumentToDocumentListConverter:\n",
    "\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, document: Document):\n",
    "        return {\"documents\": [document]}\n",
    "\n",
    "@component\n",
    "class DocumentListToDocumentConverter:\n",
    "\n",
    "    @component.output_types(document=Document)\n",
    "    def run(self, documents: List[Document]):\n",
    "        return {\"document\": documents[0]}\n",
    "\n",
    "@component\n",
    "class DocumentSectionRouter:\n",
    "\n",
    "    def __init__(self, sections: List[int]):\n",
    "        self.sections = sections\n",
    "\n",
    "    @component.output_types(rag_route=str, log_question_route=Document)\n",
    "    def run(self, document: Document):\n",
    "        if document.meta[\"classification\"] in self.sections:\n",
    "            return {\"rag_route\": document.content}\n",
    "        else:\n",
    "            return {\"log_question_route\": document}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5Mp3F8K9218"
   },
   "outputs": [],
   "source": [
    "db_persist_path = \"documents-storage\"\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "chat_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "documents = [\n",
    "    Document(content=\"Sir Winston Leonard Spencer Churchill[a] (30 November 1874 – 24 January 1965) was a British statesman, military officer, and writer who was Prime Minister of the United Kingdom from 1940 to 1945 (during the Second World War) and again from 1951 to 1955. For some 62 of the years between 1900 and 1964, he was a member of parliament (MP) and represented a total of five constituencies over that time. Ideologically an adherent to economic liberalism and imperialism, he was for most of his career a member of the Conservative Party, which he led from 1940 to 1955. He was a member of the Liberal Party from 1904 to 1924.\"),\n",
    "    Document(content=\"Michael Jeffrey Jordan (born February 17, 1963), also known by his initials MJ,[8] is an American businessman, former professional basketball player and former professional baseball player, who is currently a minority owner of the Charlotte Hornets of the National Basketball Association (NBA). He played 15 seasons in the NBA between 1984 and 2003, winning six NBA championships with the Chicago Bulls. Widely considered to be one of the greatest players of all time,[9][10][11] he was integral in popularizing basketball and the NBA around the world in the 1980s and 1990s,[12] becoming a global cultural icon.[13]\"),\n",
    "    Document(content=\"In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.[3] The modern version of the transformer was proposed in the 2017 paper 'Attention Is All You Need' by researchers at Google.[1] Transformers were first developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers).\"),\n",
    "    Document(content=\"Runner's knee, also known as patellofemoral pain syndrome (PFPS), is diagnosed through a combination of physical examination, symptom assessment, and sometimes imaging tests. A doctor will evaluate your medical history, physical activity, and symptoms to determine if it's PFPS. \"),\n",
    "    Document(content=\"The Roman Empire ruled the Mediterranean and much of Europe, Western Asia and North Africa. The Romans conquered most of this during the Republic, and it was ruled by emperors following Octavian's assumption of effective sole rule in 27 BC. The western empire collapsed in 476 AD, but the eastern empire lasted until the fall of Constantinople in 1453. By 100 BC, the city of Rome had expanded its rule from the Italian peninsula to most of the Mediterranean and beyond. However, it was severely destabilised by civil wars and political conflicts, which culminated in the victory of Octavian over Mark Antony and Cleopatra at the Battle of Actium in 31 BC, and the subsequent conquest of the Ptolemaic Kingdom in Egypt. In 27 BC, the Roman Senate granted Octavian overarching military power (imperium) and the new title of Augustus, marking his accession as the first Roman emperor. The vast Roman territories were organized into senatorial provinces, governed by proconsuls who were appointed by lot annually, and imperial provinces, which belonged to the emperor but were governed by legates.\"),\n",
    "    Document(content=\"In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs, and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.[9] By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badarian culture, which probably originated in the Western Desert; it was known for its high-quality ceramics, stone tools, and its use of copper.[10]\"),\n",
    "    Document(content=\"Tertiary structure refers to the three-dimensional structure created by a single protein molecule (a single polypeptide chain). It may include one or several domains. The α-helices and β-pleated-sheets are folded into a compact globular structure. The folding is driven by the non-specific hydrophobic interactions, the burial of hydrophobic residues from water, but the structure is stable only when the parts of a protein domain are locked into place by specific tertiary interactions, such as salt bridges, hydrogen bonds, and the tight packing of side chains and disulfide bonds. The disulfide bonds are extremely rare in cytosolic proteins, since the cytosol (intracellular fluid) is generally a reducing environment.\"),\n",
    "    Document(content=\"COVID-19 can be diagnosed through tests that detect either current or past infection. Tests for current infection, also known as viral tests, look for the presence of the virus itself, while tests for past infection, or antibody tests, detect antibodies produced in response to the virus. Common viral tests include molecular tests, such as PCR tests, which identify the virus's genetic material. \"),\n",
    "    Document(content=\"Universities can be broadly categorized into public, private, and for-profit institutions. Further classifications include liberal arts colleges, community colleges, research universities, and specialized institutions like technical schools or art colleges. Each type offers distinct educational approaches and focuses\"),\n",
    "    Document(content=\"Neural networks are broadly categorized into several types based on their architecture and function. Key types include Feedforward Neural Networks (FNNs), Recurrent Neural Networks (RNNs), and Convolutional Neural Networks (CNNs). Each type is designed for specific tasks and data types\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOgSKc86hepu"
   },
   "source": [
    "Indexing documents pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZ25rdt69-Xu",
    "outputId": "e71daf71-5f5b-4190-9d90-0898fa70abf7"
   },
   "outputs": [],
   "source": [
    "document_store = ChromaDocumentStore(persist_path=db_persist_path, collection_name=\"documents\")\n",
    "document_writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model=embedding_model)\n",
    "document_embedder.warm_up()\n",
    "\n",
    "document_classification_indexing = DocumentClassification(model_dir=save_directory)\n",
    "document_classification_indexing.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VuEE1Qr9_z-",
    "outputId": "e8227bd3-f985-4e50-d056-8aae6c5d37d0"
   },
   "outputs": [],
   "source": [
    "document_indexing = Pipeline()\n",
    "\n",
    "document_indexing.add_component(\"document_classification_indexing\", document_classification_indexing)\n",
    "document_indexing.add_component(\"document_writer\", document_writer)\n",
    "\n",
    "document_indexing.connect(\"document_classification_indexing\", \"document_writer\")\n",
    "\n",
    "document_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8h2UMlk7-Eus",
    "outputId": "7b1d10ba-4b07-49ad-b75f-e2ac7184033a"
   },
   "outputs": [],
   "source": [
    "document_indexing.run({\"documents\": documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_RBytpz-HvU",
    "outputId": "52046ea0-baf2-45a4-e11d-79f44cd52015"
   },
   "outputs": [],
   "source": [
    "def list_documents():\n",
    "    stored_docs = document_store.filter_documents()\n",
    "    return {doc.content[:100]: index_to_section[doc.meta[\"classification\"]] for doc in stored_docs}\n",
    "\n",
    "list_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkg5DLCwhixS"
   },
   "source": [
    "Question answering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9Jc3dJD-Igy",
    "outputId": "12658e6e-cf74-4035-d0ed-d2ade14d78a8"
   },
   "outputs": [],
   "source": [
    "sections = [0]\n",
    "\n",
    "query_to_document_converter = TextToDocumentConverter()\n",
    "document_classification_adapter = DocumentToDocumentListConverter()\n",
    "document_classification = DocumentClassification(model_dir=save_directory)\n",
    "document_classification.warm_up()\n",
    "document_classification_post_processor = DocumentListToDocumentConverter()\n",
    "\n",
    "document_section_router = DocumentSectionRouter(sections=sections)\n",
    "\n",
    "document_retriever = ChromaQueryTextRetriever(\n",
    "    document_store=document_store,\n",
    "    top_k=3,\n",
    "    filters={\"field\": \"classification\", \"operator\": \"in\", \"value\": sections},\n",
    ")\n",
    "\n",
    "template = [\n",
    "    ChatMessage.from_user(\n",
    "        \"\"\"\n",
    "        You are a question answering agent. Given the following context, answer the user's question.\n",
    "\n",
    "        Context:\n",
    "        {% for document in documents %}\n",
    "            {{ document.content }}\n",
    "        {% endfor %}\n",
    "\n",
    "        Question: {{ query }}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt_builder = ChatPromptBuilder(template=template, required_variables=[\"query\", \"documents\"])\n",
    "\n",
    "generator = HuggingFaceAPIChatGenerator(api_type=HFGenerationAPIType.SERVERLESS_INFERENCE_API,\n",
    "                                        api_params={\"model\": chat_model},\n",
    "                                        token=Secret.from_token(hf_token))\n",
    "\n",
    "log_writer_adapter = DocumentToDocumentListConverter()\n",
    "\n",
    "log_document_store = ChromaDocumentStore(persist_path=db_persist_path, collection_name=\"logs\")\n",
    "log_writer = DocumentWriter(document_store=log_document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlJKBTK7-Lmr",
    "outputId": "b4dc8914-a1e8-4c5e-b28e-9e0922e664f4"
   },
   "outputs": [],
   "source": [
    "rag_pipeline = Pipeline()\n",
    "\n",
    "rag_pipeline.add_component(\"query_to_document_converter\", query_to_document_converter)\n",
    "rag_pipeline.add_component(\"document_classification_adapter\", document_classification_adapter)\n",
    "rag_pipeline.add_component(\"document_classification\", document_classification)\n",
    "rag_pipeline.add_component(\"document_classification_post_processor\", document_classification_post_processor)\n",
    "rag_pipeline.add_component(\"document_section_router\", document_section_router)\n",
    "\n",
    "rag_pipeline.add_component(\"document_retriever\", document_retriever)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component(\"generator\", generator)\n",
    "\n",
    "rag_pipeline.add_component(\"log_writer_adapter\", log_writer_adapter)\n",
    "rag_pipeline.add_component(\"log_writer\", log_writer)\n",
    "\n",
    "rag_pipeline.connect(\"query_to_document_converter\", \"document_classification_adapter\")\n",
    "rag_pipeline.connect(\"document_classification_adapter\", \"document_classification\")\n",
    "rag_pipeline.connect(\"document_classification\", \"document_classification_post_processor\")\n",
    "rag_pipeline.connect(\"document_classification_post_processor\", \"document_section_router\")\n",
    "\n",
    "rag_pipeline.connect(\"document_section_router.rag_route\", \"document_retriever.query\")\n",
    "rag_pipeline.connect(\"document_section_router.rag_route\", \"prompt_builder.query\")\n",
    "rag_pipeline.connect(\"document_retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "rag_pipeline.connect(\"document_section_router.log_question_route\", \"log_writer_adapter\")\n",
    "rag_pipeline.connect(\"log_writer_adapter\", \"log_writer\")\n",
    "\n",
    "rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9G2m5gLX-OEv",
    "outputId": "b1d60a69-e28c-4c6b-eb1c-8a2bec09b92b"
   },
   "outputs": [],
   "source": [
    "rag_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFwqzHI0-QD4",
    "outputId": "019b1af1-86be-4030-c5c2-19871a8edf90"
   },
   "outputs": [],
   "source": [
    "question = \"When did the Roman Empire fall?\"\n",
    "\n",
    "response = rag_pipeline.run(\n",
    "    {\"query_to_document_converter\": {\"text\": question}},\n",
    "    include_outputs_from=[\"document_classification\"]\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Wb27spsK-S5J",
    "outputId": "1b48660b-c588-4a07-c75c-dfc87c5b6371"
   },
   "outputs": [],
   "source": [
    "response[\"generator\"][\"replies\"][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VVLyKTz-UEl",
    "outputId": "e5d97d5e-cbba-42d8-ea61-9d456bed6ebf"
   },
   "outputs": [],
   "source": [
    "question = \"How do i know if I have runner's knee?\"\n",
    "\n",
    "response = rag_pipeline.run(\n",
    "    {\"query_to_document_converter\": {\"text\": question}},\n",
    "    include_outputs_from=[\"document_classification\"]\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqgWyZBz-WUu"
   },
   "outputs": [],
   "source": [
    "def get_unanswered_questions():\n",
    "    documents = log_document_store.filter_documents()\n",
    "    return [document.content for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XZGit-n-ahH",
    "outputId": "0ba6ef9c-3b2c-4b82-a983-55c785492e34"
   },
   "outputs": [],
   "source": [
    "get_unanswered_questions()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
